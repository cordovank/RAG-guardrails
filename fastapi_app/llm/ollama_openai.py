"""
**********************************
* DO NOT EDIT THIS FILE DIRECTLY *
**********************************

This file is used as default LLM for the RAG pipeline.
It provides a callable function to interact with Ollama via an OpenAI-compatible API endpoint.
Requires ollama installed and running locally or accessible via network.

If you wish to use a different LLM, please modify the settings in `fastapi_app/settings.py` under
the "CUSTOM LLM settings" section.

"""

# fastapi_app/llm/ollama_openai.py
import os
from typing import Optional, Callable
from openai import OpenAI

from fastapi_app.settings import settings as set

# Load settings from app settings
OLLAMA_API_KEY = set.ollama_api_key
OLLAMA_BASE_URL = set.ollama_base_url
OLLAMA_MODEL_LLAMA = set.ollama_model_llama


def _create_client(api_key: str = OLLAMA_API_KEY, base_url: str = OLLAMA_BASE_URL) -> OpenAI:
    # Construct an OpenAI client that talks to Ollama's OpenAI-compatible endpoint
    return OpenAI(base_url=base_url, api_key=api_key)

def get_llm_callable(model: Optional[str] = None, timeout: int = 30) -> Callable[[str], str]:
    """
    Return a tiny callable llm_fn(prompt) -> str that uses Ollama via OpenAI-compatible API.
    Usage:
        llm_fn = get_llm_callable()
        synthesize_answer(..., llm_fn=llm_fn)
    """
    client = _create_client()
    chosen_model = model or OLLAMA_MODEL_LLAMA

    def llm_fn(prompt: str, max_tokens: int, temp: float = 0.0) -> str:
        resp = client.chat.completions.create(
            model=chosen_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temp,
            max_tokens=max_tokens,
            timeout=timeout,
        )

        return resp.choices[0].message.content
       
    return llm_fn